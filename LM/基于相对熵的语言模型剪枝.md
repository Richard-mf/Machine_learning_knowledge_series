# 基于相对熵的语言模型剪枝

## 1. 目的

- 缩减语言模型大小，一般高阶语言模型不做裁剪会有十几到几十个G的大小，在实际应用中内存承受不了
- 去除冗余信息，使高效快速

## 2. 原理

剪枝的主要目的是为了删除已有ngram，同时保证未删除的ngram不变，还需要重新计算回退概率。   那么怎么样衡量剪枝以后语言模型性能的改变呢？   *一个想法*就是去最小化剪枝前后两个模型之间概率分布的距离。自然的，我们会选择相对熵或者KL距离。

见相关资料：

[语言模型srilm（一） 基本用法](https://blog.csdn.net/xmdxcsj/article/details/50353689)

[语言模型srilm（二） prune剪枝](https://blog.csdn.net/xmdxcsj/article/details/50321613)

[Entropy-based Pruning of Backoff Language Models](https://www.sri.com/sites/default/files/publications/entropy-based_pruning_of_backoff_language_models.pdf)

# 3. srilm 剪枝操作

```shell
ngram -lm {oldlm} -order 2 -prune {thres} -write-lm ${newlm} 
# -prune threshold 删除一些ngram，满足删除以后模型的ppl增加值小于threshold，越大剪枝剪得越狠 
# -write-lm 新的语言模型
# -order 2 语言模型最大阶数为2
```

裁剪后的性能比较见如下表格：

![](/home/mf34/Documents/GitHub/Machine_learning_knowledge_series/LM/prune1.png)

## 4. 小结

由以上第3节的表格可知一般的prune的裁剪取值范围为：1e-9,1e-8,1e-7。